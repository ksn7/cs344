{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Proposal 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the work Professor VanderLinden is doing in analyzing tweets and classifying them as positive or negative, I would like to use the IMDB movie review dataset to build a statistical filter that would classify movie reviews as positive or negative. I chose this dataset because I think natural language processing is fascinating, but since it is rather complicated a simple classification project like this one should be manageable. I would be interested to learn based on the outcome of this filter what words actually correlate with positive and negative reviews and if there are any words that are surprising in both sets. My concern in choosing this dataset is that the words are preprocessed into a set of embeddings, so I'm not sure if I can access the words themselves to build my filter. If that is not the case, I may have to choose another dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am interested in attempting this task with the statistical method because I have great personal interest in mathematics, causing me to really believe in the statistical method. However, reviews are likely to be quite complicated and I'm not entirely convinced that the statistical method will do as good of a job as perhaps a neural network would do. Thus, my interest is in seeing how good of a job a statistical method will do at classifying reviews as positive or negative. I am hopeful that it will do a good job, showing that a simple, straightforward approach to natural language processing is possible for some situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposal 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our recent coursework has made me interested in Prolog, a programming language known for its use in logic. For my project, I would like to learn more about Prolog and its uses by independently studying chapter 10 of the \"Learn Prolog Now\" online book by Blackburn, Bos, and Striegnitz. This chapter discusses cuts and negation, which relate to Prolog's built in backtracking algorithm and how it can be controlled. I have not had much opportunity to yet look into this chapter in detail, but I think that this topic will be interesting to me and potentially useful. The first part of my plan will to be study this chapter in depth and complete its exercises, then report my most important and interesting findings. I would also be interested in looking at other sources to know more about the applications of Prolog in general, especially when employing the use of the cut and negation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background, Implementation, and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way that Prolog makes its logic decisions reminds me of decision tree learning, which we discussed earlier in the course. I am proposing that following my study of LPN Chapter 10, I relate my findings to the use of decision tree learning. This applies directly to the AI field of information retrieval and extraction, although at a very basic level. I hope to gain a good understanding of both techniques, then relate them, explain their similarities and differences, and if possible suggest ways to integrate their techniques. I especially here would like to play around with the idea of entropy and information gain, perhaps using some of Prolog's arithmetic abilities to calculate these values and integrate that into where and how I use the cut. I would likely need to read the arithmetic section of the LPN text to understand how this integration would work if even possible, so this could be a part of my project as well. Again, I haven't yet learned much about this use in Prolog, but I would like to explore these ideas and report on my findings in lieu of a more code-heavy project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context, it is difficult to know exactly what my implementation will look like. I plan to include some basic examples of what the cut is and how it is useful, which could be in the form of basic prolog knowledge bases and queries. Should my idea to integrate with decision tree learning work out, I would include some additional demonstrations of what this looks like, perhaps inlcuding some hand-drawn or digital decision trees. If the ideas integrate, I would discuss how they connect, then include a prolog file with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps regardless of how well this works out, my results would be an explanation of what I did, why it is interesting, and what did or did not work. If the ideas do not integrate, this could primarily be a few paragraphs comparing and contrasting how the cut and Prolog are structured as opposed to decision tree learning, where they are used, and when one is a more useful technology than the other. I hope to find some sources that give me good grounds for this section, along with my own thoughts. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be interested in thinking more about the implications of using Prolog to make decisions. Prolog of course can only make logic deductions based on what it is given in the first place. I think the use of it for machine learning then is when it is given a high volume of information that humans have a difficult time sorting through and understanding. The advantage to us is that Prolog can make logical deductions quickly and easily, and we can trust that if the information it was given is correct, the output it returns is also correct. However, it could be difficult to track what pieces of information Prolog is using to make its deductions given a large volume of data, which makes it less trustworthy in a time when it is important to be transparent about how and when data is being used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, my time in the CS capstone course have made me concerned about the modern tendency to turn all decisions into objective decisions that a computer can make, a place where Prolog could be actively employed. We seem to think that if we supply a computer with information and it makes a decision, the decision must be unbiased because the computer can't really think. In other contexts, my concern would be that a model like a neural network would accidentally learn a bias and use it in its decision making process. However, here I am more concerned with the movement to make all of these decisions objective. A person could build a Prolog knowledge base and use it to make decisions that are actually unsound simply because the decision is actually subjective and too complex to make an objective decision about. This reductionist view of the world is dangerous in many ways, and I would be interested in discussing my views on it further for the final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
