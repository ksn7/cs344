{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 1: Essay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I believe that deep neural networks are a useful advancement that will last for a few years, but it is difficult for me to say that they will live on for the forseeable future given the historic eventual failure of so many AI advancements. Throughout the history of AI, each significant advancement has been widely celebrated for a few years, until it proves to be limited in scope or to produce unreliable results. For example, at one point expert systems were thought to be the way of the future, but they no longer exist today because we found that they didn't do as good of a job as they claimed. Another example is perceptrons, which were a huge breakthrough in early AI. We still use the underlying ideas of perceptrons, but their basic use has run its course because it was discovered that perceptrons could only handle a few limited computations. This has happened over and over in AI, and although deep neural networks seem like a huge breakthrough, they very well may follow down the same path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the future that I imagine, deep neural networks will stick around for the next few years. Although their reign may be temporary, I think that for modern use deep neural networks are the best tool we have for modelling a number of scenarios and situations. I see the success of DNNs in the photo recognition software that Google has been able to achieve, where when I go to look at my Google Photos I can search for people, places, or objects and Google will find the picture that I'm looking for. This sort of use for DNNs I think will stick around for a longer term. Another success of DNNs is in television and movie recommendations, which are used by a number of platforms and which I think most peple would say have been valuable. These functions are useful and popular, and I think their use will stick around for a while."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One may notice that the functions listed above are useful to individuals on an entertainment and personal use level, and I think that over time these will be the only uses of deep neural networks that last. For many other purposes, I think that the general public will eventually determine that DNNs cannot do a good job of representing them because of a lack of complexity and human factors. An example I think of quickly was given by a faculty candidate earlier this semester. In a seminar presentation, he spoke over and over again of when hiring algorithms produced results that were racially biased, or of algorithms that determined certain neighborhoods to be at higher risk for domestic abuse because of low incomes. DNNs can be applied to many useful situations, like estimating credit or evaluating markets, but there is little control over what factors they will determine to be important, producing results that could be horribly biased. The fact is, suggesting that a computer or algorithm could make decisions that require human factors is a form of reductionism, which I believe to be wrong based on my Christian faith. The world and society we live in cannot be properly represented by 1s and 0s, and suggesting that an algorithm could use those digits to make decisions that affect human lives in the long term is wrong on the scale of naive to ignorant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Back propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Work done by hand, attached in homework4_1.png and homework4_2.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0948 - acc: 0.9619\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 76s 1ms/step - loss: 0.0617 - acc: 0.9754 - ETA: 2s - los\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 84s 1ms/step - loss: 0.0527 - acc: 0.9792\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 94s 2ms/step - loss: 0.0471 - acc: 0.9813\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 113s 2ms/step - loss: 0.0432 - acc: 0.9829\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 138s 2ms/step - loss: 0.0399 - acc: 0.9843\n",
      "10000/10000 [==============================] - 7s 721us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.052884889633953575, 0.9787900047302246]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' The following code was borrowed and altered from the in-class example,\n",
    "    found in keras-cnn.ipynb'''\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "# Configure a convnet with 3 layers of convolutions and max pooling.\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Add layers to flatten the 2D image and then do a 10-way classification.\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=6, batch_size=75)\n",
    "model.evaluate(test_images, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
